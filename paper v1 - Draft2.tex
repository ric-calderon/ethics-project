% ┌────────────────────────────────────────────────────────────────────┤ • Preamble • ├
\documentclass[11pt]{amsart}
\usepackage{unicode-math}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{wrapfig}
\usepackage[rightcaption]{sidecap}
\usepackage{float}
\usepackage[font=small, labelfont=bf, skip=5pt, margin=10pt]{caption}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=green]{hyperref}

\addtolength{\oddsidemargin}{-.55in}
\addtolength{\evensidemargin}{-.55in}
\addtolength{\textwidth}{1.1in}

\addtolength{\topmargin}{-.25in}
\addtolength{\textheight}{.5in}

\newcounter{myFigure}

% The option fixes a problem with ligatures preventing from searchable text.
\setmainfont[Ligatures=NoCommon]{Cambria}
% \setsansfont{Verdana}[Scale=0.80]
\setsansfont{Arial}[Scale=0.80]

% Reduce space above title
% Redefine \@settitle to adjust spacing
\makeatletter
\def\@settitle{%
\begin{center}%
\normalfont\bfseries
\vspace*{-3cm} % Adjust this value for spacing above the title
\huge
\@title
\vspace{0.5cm} % Adjust this value for spacing below the title
\end{center}%
}
\makeatother
% └──────────────────────────────────────────────────────────────────────────────────────

\begin{document}

% ┌─────────┤ • ├
\title{On Privacy in Everyday Location Data} % 👉
\author{Ricardo Calderon}
\maketitle
\begin{abstract}
    This study investigates the performance of various machine learning models in predicting human movement based on sequences of GPS location data.
    The models evaluated include Random Forest (RF) Regression, Neural Networks (NN), Support Vector Regression (SVR), and Hidden Markov Models (HMM).
    To address the privacy risks of location data, the study focuses on anonymization methods that reduce predictive value.
\end{abstract}
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────┤ Introduction ├╌╌╌╌╌╌
\section{Introduction}

% ┌─────────┤ • ├
At a societal level, the use of location data generally provides significant benefits, such as optimizing public infrastructure, improving public health, and enhancing emergency response efforts.
However, the misuse of location data---whether for state surveillance or personal stalking---raises serious concerns at the individual level, highlighting the need for stronger protections.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\vspace{-.25cm}
\includegraphics[width=\linewidth]{routes.pdf} % Slightly smaller width to ensure proper wrapping
\caption{\small Visited locations in Chicago, with dots representing collected locations and route segments shown in different colors.}
\label{fig:routes}
\end{wrapfigure}

This paper evaluates the performance of various machine learning techniques in using a sequence of movements to predict the next move taken and quantifies unpredictability in everyday movement through an entropy analysis.

The broader goal is to explore methods for anonymizing location data by reducing its predictive value.
To make the data worse for predictions, we must first quantify the quality of the information it contains.
\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{Shannon's information theory} introduces the concept of entropy, which allows us to quantify the missing information in a sequence of data, but this requires us to associate probabilities with each state.
It is possible to extract these probabilites from the transition matrix generated by a \href{https://en.wikipedia.org/wiki/Hidden_Markov_model}{Hidden Markov Model}.

This paper is organized as follows: The next section has a detailed explanation of the data collected to ease peer review.
The subsequent Analysis section discusses all the models tested and their results.
The Conclusion section has recomendations for anonimization techniques.
Finally, the appendix includes a glossary of essential terms that are important to the paper but were not covered in class.

Overall, the paper contributes to the conversation about how to responsibly use technology while protecting individual privacy and preventing misuse of data.

{\color{white}Line}
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────┤ The data set ├╌╌╌╌╌╌
\section{The data set}

% ┌─────────┤ • ├
Location data is inherently sensitive and can often be de-anonymized with minimal effort.
For instance, a brief examination of Fig.~\ref{fig:routes} would likely reveal my association with the University of Chicago.
Including timestamps in the dataset could further expose personal details such as my home address, class schedule, or preferred dining locations.
Thus, to avoid handling data that would be extremely difficult to anonymize, I chose to work exclusively with my own location data.

The collection of location data was automated through a \href{https://play.google.com/store/apps/details?id=net.dinglisch.android.taskerm&hl=en_US}{Tasker} script on my phone.
Every 120 seconds the script would collect my longitude and latitude, coordinate accuracy, bearing, bearing accuracy, and timestamp.
Because of hardware limitations, only longitude, latitude, and timestamps were consistently recorded.
For redundancy's sake, this data was sent as a text message to myself and written to a local .txt file.

The data sequences included examples of walking and taking public transportation downtown, driving across the city to run errands, walking to class, and running errands in the neighborhood.
I sought to give examples of typical movements people could take while using the three most common means of transportation: walking, public transport, car.

Data sequences were collected on 5 different days and tagged with a group number.
However, the location collection would occasionally fail, commonly while walking through parks and when near Lake Michigan.
If these collection failures resulted in consecutive measurements being more than 12 minutes apart or more than 30 meters away, the sequence would be split into separate segments.

Originally, each preimage was a single coordinate pair and the image was the next coordinate pair but this led to poor performance with every model type other than a Random Forest Regression.
By replacing the preimage pairs with subsequences of consecutive pairs, we introduced \emph{memory} to the models, allowing for Neural Networks, Support Vector Regressions, and Hidden Markov Models to be applied on the data.
Segments were split into successive subsequences with $k$ elements, to train and test the models.
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────────┤ Analysis ├╌╌╌╌╌╌
\section{Analysis}

% ┌─────────┤ • ├
At first, instead of making a model that returns a coordinate vector, I trained paired models to predict each coordinate individually.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Model & RF (k=1) & RF (k = 3) & NN (k=1) & NN (k = 3) & SVR (k=1) & SVR (k = 3)\\ \hline
Latitude RMSE & 0.00860 & 0.00670 & 5467.96 & 0.34370 & 0.06478 & 0.06299 \\ \hline
Longitude RMSE & 0.00423 & 0.00426 & 8468.09 & 1.24100 & 0.02146 & 0.02167 \\ \hline
\end{tabular}
\caption{Models to predict the next latitude and longitude given the current latitude and longitude.
Each coordinate was computed independently.}
\label{table1}
\end{table}

However, I later realized that my 2d preimages should be mapped to 2d images, so I tested models that could take and return vectors.
Moreover, the models that I had tried would not help me further the goal of finding which trips would reduce the predictive value of the input because none of them provided a way to compute the probabilities needed for Eq.~\ref{entropyeq}.
Thus, I added HMMs to the mix.
Using the rows of a HMM's transition matrix, one can calculate the transition entropy, see Eq.~\ref{transition_entropy}.
Let $s$ be the intial hidden state.
The transition entropy of $s$ is the information missing when transitioning out of $s$, using the correspoding $s$-th row of the transition matrix.
The average over all rows is the average information missing if you do not know which state you are transitioning out of.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Model & RF (k = 3) & NN (k = 3) & HMM (k=1) \\ \hline
RMSE & 0.00649 & 1.9147 & 1.494\\ \hline
\end{tabular}
\caption{Models to predict the next latitude and longitude given the current latitude and longitude.
The features are 2d points and so are the predictions.}
\label{table2}
\end{table}

As noted in Table \ref{table2}, the HHM performed better than the NN but several orders of magnitude worse than the RF model.
To find out why, I varied the number of internal states, see Fig.~\ref{fig:rmse_entropy}.
Surprisingly the RMSE appeared independent from the number of hidden states, which could be blamed on the small size of the dataset.
When there are not enough data points to support the internal states, the free scalar parameters will outnumber the data points, resulting in a degenerate solution.
In Fig.~\ref{fig:rmse_entropy}, this occured for models with more than 19 hidden states.
When this happens, the models stop converging altogether, so entropy and accuracy metrics lose their meaning.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{not_enough.pdf} % Adjust width as needed
    \caption{Markov model without memory.
    The personal location data is so sparse that the HMM cannot compute transition probabilities.}
    \label{fig:rmse_entropy}
\end{figure}
% with memory

Further research revealed that HMMs can perform better when provided with memory since their features can incorporate both the current observation and some previous states into their predictions.
The data in Fig.~\ref{fig:hmm_memory} was computed by increasing the length of the subsequences provided as preimages.
Unlike the previous models, the number of hidden states was kept constant.
\begin{figure}[b]
    \centering
    \includegraphics[width=0.5\textwidth]{RMSE_and_Entropy_at_9_hidden_states.pdf} % Adjust width as needed
    \caption{Markov model with memory.
    RMSE seems to decrease as the sequence size increases, although the RMSE becomes volatile when the sequence size is large for the dataset.}
    \label{fig:hmm_memory}
\end{figure}
Transition entropy decreases slowly as sequence size increases, indicating that the model is fitting better (missing less information).
The decreasing RMSE supports this conclusion.
Note that the HMM models had trouble converging once the sequence sizes passed 23.
% This might explain the oscillatory behavior present in Fig \ref{fig:hmm_memory}.

The models in Fig.~\ref{fig:hmm_memory} have 9 hidden states which is equivalent to dividing Chicago into 9 regions.
This was fine when investigating the effect of varying the sequence size, but the resolution of the model has to be increased.
In the models of Fig.~\ref{fig:hmm_hidstate}, the number of hidden states was tied to the size of the sequence.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{RMSE_and_Entropy_variable_hidden_states.pdf} % Adjust width as needed
    \caption{
    RMSE decreases and average transition entropy increase as the sequence size increases.}
    \label{fig:hmm_hidstate}
\end{figure}
As the sequence size was increased to 15 and the number of hidden states was increased to 7, the performance of the model steadily improved.
Past this point, the size of the dataset again becomes an issue.
Nevertheless, the observed increase in entropy requires some explanation.
My educated guess is that by increasing the number of regions in which the space is divided, we increase the probability of missing information.
To maximize Eq \ref{transition_entropy}, each state must be independent of the previous state.
Assuming independence allows us to use Eq \ref{entropyeq}, which in turn is maximized when every state is equally likely.
% That is, when the sequence is totally random, entropy takes its greatest value.
If length of the sequence is $k$, then each $p( x ) = \frac 1 k$ and the maximum entropy is $\log_2( k )$
Hence, the maximum entropy increases as $k$ increases.
However, increasing the entropy does not necessarily mean the model's performance will degrade; in fact, for the region supported by the size of the dataset, the performace improves.
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌────────────────────────────────────────────────────────────────┤ Conclusion ├╌╌╌╌╌╌
\section{Conclusion}

% ┌─────────┤ • ├
The limitations of this analysis stemmed from practical constraints rather than the inherent capabilities of Hidden Markov Models (HMMs).
To ensure data redundancy during collection, recordings were made every two minutes---the shortest interval permitted by Tasker without significantly impacting battery life.
Despite these challenges, the analysis yielded valuable insights.

Among the models tested, the RF Regression had the lowest RMSE across all input and parameter configurations.
After converting the RMSE from coordinates into distance, our best RF model had an average RMSE of 0.722 km, or about 7 city blocks.
This is not precise enough to track someone, but given the very limited data, it is satisfactory as a proof of concept.
The next best model (an HMM) had an RMSE several order of magnitude larger.
Increasing the subsequence length vastly improved HMM and NN performance, but dataset cardinality limited the extent of these improvements.

Back to our goal of providing anonymizing strategies, if you are in a state whose corresponding row in the transition matrix has a high entropy, then the model is less certain about your next move.
In other words, you can anonymize yourself by ensuring your routes take you through locations with high transition entropy.
For example, if I am spotted leaving my apartment, I am either going south to campus or north to the Roosevelt Red Line station.
However, from the station there is a nearly equal probability that I'll walk in any direction, so being spotted at the station does not carry the same information as getting spotted leaving my apartment (recall the previous explanation of how to maximize entropy).

To maximize entropy, one might adopt a "hub-and-spoke" system similar to those used by airlines, where travel is routed through high-entropy hubs, even at the cost of increased travel time.
These hubs, identified as locations with the highest transition entropy, act as points where anonymity is naturally preserved by increasing uncertainty in the prediction of subsequent movements.

Interestingly, nature has already figured this out: fish mitigate predation risk by gathering in schools, diluting the likelihood of any one individual being targeted.
Similarly, adopting routes through high-entropy hubs could help individuals reduce the risks associated with the predictability of their movements.
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────────┤ Appendix ├╌╌╌╌╌╌
\section*{Appendix}

% ┌─────────┤ • ├
A Hidden Markov Model (HMM) is a statistical model used to describe systems that have observable data (outputs) that depend on internal states, which are hidden and not directly observable.
The following is a list of the key concepts in HMM.

\textbf{States}: A set of hidden states that the model transitions between over time.
The sequence of these states represents the internal structure of the process but is not directly observed.

% • Example: In nuclear physics, we do not directly observe the states of protons and neutrons within the nucleus, such as their spin orientations or energy levels.
% Instead, these hidden states are inferred from experimental data, like emitted radiation.

\textbf{Observations}: The observable data generated by the system.
These are related to the hidden states, but the relationship is probabilistic.

% • Example: We observe gamma rays or beta particles emitted or absorbed by a nucleus.
% These emissions are influenced by the hidden quantum states of the nucleus (e.g., excited states or decay modes).

\textbf{Transition Probabilities}: The probabilities of moving from one hidden state to another.

% • Example: A nucleus in a specific excited state can probabilistically transition to another state, such as through proton-neutron conversion during beta decay.
% Inside a stable nucleus, this probability is finite, but for a free proton in isolation, the transition probability is so low that it practically never occurs within the age of the universe.

\textbf{Emission Probabilities}: The probabilities of observing a specific output given a particular hidden state.

% • Example: If the nucleus is in a specific excited state, there is a certain probability that it will emit gamma radiation of a specific energy.
% These probabilities are determined by the nuclear structure and selection rules in quantum mechanics.

\textbf{Initial Probabilities}: The probabilities of the system starting in each of the hidden states.

% • Example: The likelihood of a nucleus starting in an excited state versus a ground state can depend on how the nucleus was formed, such as through a nuclear reaction or radioactive decay.

What follows are the relevant definitions used in the paper's entropy discussion.

\textbf{Shannon Entropy}: measures the average uncertainty in a probability distribution of states.
\begin{equation}
    \label{entropyeq}
    H( X ) ≔ - \sum_{x ∈ X}p( x )\log_2 p( x ),
\end{equation}
The entropy of a sequence represents the average amount of information that could be encoded by the sequence.
For example, four letter words in English could take 26 states per letter, so there are $26^4$ possible permutations.
However, not every permutation is equally likely.
There are more consonants than vowels, so a word with 3 consonants and 1 vowel is less surprising than a word with 1 consonant and 3 vowels.
That is, not every sequence carries the same amount of information.
We want to compare the entropy of "everyday" location sequences to completely random location sequences to see how unpredictable an average person is.
After all, the higher the entropy, the lower the amount of information that can be extracted from the sequence.
This is a consequence of the definition of entropy as the average amount of uncertainty in a state.

\textbf{Transition Entropy}: a specific form of entropy that measures uncertainty associated with transitioning from one state to the other states in a system.
\begin{equation}
    \label{transition_entropy}
    H_{transition} = -\sum_i p( s_i )\sum_j p( s_j|s_i )\log_2 p( s_j|s_i )
\end{equation}
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌

\end{document}
