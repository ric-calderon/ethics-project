% ┌────────────────────────────────────────────────────────────────────┤ • Preamble • ├
\documentclass[11pt]{amsart}
\usepackage{unicode-math}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{wrapfig}
\usepackage[rightcaption]{sidecap}
\usepackage{float}
\usepackage[font=small, labelfont=bf, skip=5pt, margin=10pt]{caption}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=green]{hyperref}

\addtolength{\oddsidemargin}{-.55in}
\addtolength{\evensidemargin}{-.55in}
\addtolength{\textwidth}{1.1in}

\addtolength{\topmargin}{-.25in}
\addtolength{\textheight}{.5in}

\newcounter{myFigure}

% The option fixes a problem with ligatures preventing from searchable text.
\setmainfont[Ligatures=NoCommon]{Cambria}
% \setsansfont{Verdana}[Scale=0.80]
\setsansfont{Arial}[Scale=0.80]

% Reduce space above title
% Redefine \@settitle to adjust spacing
\makeatletter
\def\@settitle{%
\begin{center}%
\normalfont\bfseries
\vspace*{-3cm} % Adjust this value for spacing above the title
\huge
\@title
\vspace{0.5cm} % Adjust this value for spacing below the title
\end{center}%
}
\makeatother
% └──────────────────────────────────────────────────────────────────────────────────────

\begin{document}

% ┌─────────┤ • ├
\title{On Privacy in Everyday Location Data} % 👉
\author{Ricardo Calderon}
\maketitle
\begin{abstract}
    This study investigates the performance of various machine learning models in predicting human movement based on sequences of GPS location data.
    The models evaluated include Random Forest Regression, Neural Networks, Support Vector Regression, and Hidden Markov Models.
    To address the privacy risks of location data, the study focuses on anonymization methods that reduce predictive value.
\end{abstract}
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────┤ Introduction ├╌╌╌╌╌╌
\section{Introduction}

% ┌─────────┤ • ├
This paper aims to measure the performance of various machine learning techniques in using a sequence of movements to predict the next move taken and to quantify the level of unpredictability present in everyday movement through an entropy analysis.

The broader goal is to explore methods to anonymize location data by reducing its predictive value.
To make the data worse for predictions we must first quantify the quality of information.
\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{Shannon's information theory} introduces the concept of entropy, which allows us to quantify the missing information in a sequence of data, but this requires us to associate probabilities with each state.
It is possible to extract these probabilites from the transition matrix generated by a Hidden Markov Model.

\begin{wrapfigure}{r}{0.35\textwidth}
\centering
\vspace{-.75cm}
\includegraphics[width=\linewidth]{routes.pdf} % Slightly smaller width to ensure proper wrapping
\caption{\small Visited locations in Chicago, with dots representing collected locations and route segments shown in different colors.}
\label{fig:routes}
\end{wrapfigure}

❖To do:
Location data is inherently sensitive and can often be de-anonymized with minimal effort.
For instance, a brief examination of Fig.~\ref{fig:routes} would likely reveal my association with the University of Chicago.
Including timestamps in the dataset could further expose personal details such as my home address, class schedule, or preferred dining locations.
Thus, to avoid handling data that would be extremely difficult to anonymize, I chose to work exclusively with my own location data.

In the next section, we will explore the motivation behind anonymizing location data, the challenges it presents, and the benefits and drawbacks of collecting such data.
❖To do: That sets the stage for understanding the technical opportunities for anonymization.
The methodology section provides a detailed explanation of the data collected, ensuring that the analysis done in the subsequent section can be peer-reviewed.
Finally, the appendix includes a glossary of terms: essential definitions for concepts foundational to the paper, but not covered in class.

Overall, the paper contributes to the conversation about how to responsibly use technology while protecting individual privacy and preventing misuse of data.

{\color{white}Line\\ Line}
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌───────────────────────────────────────────────────────────────┤ Methodology ├╌╌╌╌╌╌
\section{Methodology}

% ┌─────────┤ • ├
The collection of location data was automated through a \href{https://play.google.com/store/apps/details?id=net.dinglisch.android.taskerm&hl=en_US}{Tasker} script on my phone.
Every 120 seconds the script would collect my longitude and latitude, coordinate accuracy, bearing, bearing accuracy, and timestamp.
Only longitude, latitude, and timestamps were consistently recorded.
For redundancy's sake, this data was sent as a text message to myself and written to a local .txt file.

The data sequences included examples of walking and taking public transportation downtown, driving across the city to run errands, walking to class, and running errands in the neighborhood.
I sought to give examples of typical movements people could take while using the three most common means of transportation: walking, public transport, car.

Data sequences were collected on 5 different days and tagged with a group number.
However, the location collection would occasionally fail, commonly while walking through parks and when near Lake Michigan.
If these collection failures resulted in consecutive measurements being more than 12 minutes apart or more than 30 meters away, the sequence would be split into separate segments.

Originally, each preimage was a single coordinate pair and the image was the next coordinate pair but this led to poor performance with every model type other than a Random Forest Regression.
By replacing the preimage pairs with subsequences of pairs, we introduced "memory" to the models, allowing for Neural Networks, Support Vector Regressions, and Hidden Markov Models to be applied on the data.
Segments were split into successive subsequences with $k$ elements, to train and test the models.
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────────┤ Analysis ├╌╌╌╌╌╌
\section{Analysis}

% ┌─────────┤ • ├
❖To do: introduce models (how many components, configurations, etc)

At first, instead of making a model that returns a coordinate vector, I trained paired models to predict each coordinate individually.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Model & RF (k=1) & RF (k = 3) & NN (k=1) & NN (k = 3) & SVR (k=1) & SVR (k = 3)\\ \hline
Latitude RMSE & 0.00860 & 0.00670 & 5467.96 & 0.34370 & 0.06478 & 0.06299 \\ \hline
Longitude RMSE & 0.00423 & 0.00426 & 8468.09 & 1.24100 & 0.02146 & 0.02167 \\ \hline
\end{tabular}
\caption{Models to predict the next latitude and longitude given the current latitude and longitude.
Each coordinate was computed independently.}
\label{table1}
\end{table}

However, I later realized that my 2d preimages should be mapped to 2d images, so I tested models that could take and return vectors.
Moreover, the models that I had tried would not help me further the goal of finding which trips would reduce the predictive value of the input because none of them provided a way to compute the probabilities needed for Eq.~\ref{entropyeq}.
Thus, I added HMMs to the mix.
Using the rows of a HMM's transition matrix, we can calculate the transition entropy, see Eq.~\ref{transition_entropy}.
This entropy is the information missing when transitioning out of the internal state correspoding to that row in the transition matrix.
The average over all rows is the average information missing if you do not know which state you are transitioning out of.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Model & RF (k = 3) & NN (k = 3) & HMM (k=1) \\ \hline
RMSE & 0.00649 & 1.9147 & 1.494\\ \hline
\end{tabular}
\caption{Models to predict the next latitude and longitude given the current latitude and longitude.
The features are 2d points and so are the predictions.}
\label{table2}
\end{table}

As noted in Table \ref{table2}, the HHM performed better than the NN but several orders of magnitude worse than the RF model.
To find out why, I varied the number of internal states, see Fig.~\ref{fig:rmse_entropy}.
Surprisingly the RMSE appeared independent from the number of hidden states, which could be blamed on the small size of the dataset.
When there are not enough data points to suppor the internal states, the free scalar parameters will outnumber the data points, resulting in a degenerate solution.
In Fig.~\ref{fig:rmse_entropy}, this occured for models with more than 19 hidden states.
When this happens, the models stop converging altogether, so entropy and accuracy metrics lose their meaning.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{not_enough.pdf} % Adjust width as needed
    \caption{Markov model without memory.
    The personal location data is so sparse that the HMM cannot compute transition probabilities.}
    \label{fig:rmse_entropy}
\end{figure}
% with memory

Further research revealed that HMMs can perform better when provided with memory since their features can incorporate both the current observation and some previous states into their predictions.
The data in Fig.~\ref{fig:hmm_memory} was computed by increasing the length of the subsequences provided as preimages.
Unlike the previous models, the number of hidden states was kept constant.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{RMSE_and_Entropy_at_9_hidden_states.pdf} % Adjust width as needed
    \caption{Markov model with memory.
    RMSE seems to decrease as the of the memory increases, although the RMSE becomes volatile when the sequence size is large for the dataset.}
    \label{fig:hmm_memory}
\end{figure}
Transition entropy decreases slowly as sequence size increases, indicating that the model is fitting better (missing less information).
The RMSE reflects this.
Note that the HMM models had trouble converging once the sequence sizes passed 23.
% This might explain the oscillatory behavior present in Fig \ref{fig:hmm_memory}.

The models in Fig.~\ref{fig:hmm_memory} have 9 hidden states which is equivalent to dividing Chicago into 9 regions.
This was fine when investigating the effect of varying the sequence size, but the resolution of the model has to be increased.
In the models of Fig.~\ref{fig:hmm_hidstate}, the number of hidden states was tied to the size of the sequence.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{RMSE_and_Entropy_variable_hidden_states.pdf} % Adjust width as needed
    \caption{❖To do: .
    RMSE seems to decrease as the of the memory increases, although the RMSE becomes volatile when the sequence size is large for the dataset.}
    \label{fig:hmm_hidstate}
\end{figure}
Increasing the sequence size to 15 and the number of hidden states to 7, the performance of the model steadily increased.
Past this point, the size of the dataset again becomes an issue.
Nevertheless, up to sequence size 15, the entropy does increase, and this requires some explanation.
My educated guess is that by increasing the number of regions in which the space is divided, we increase the probability of missing information.
To maximize Eq \ref{transition_entropy}, each state must be independent of the previous state.
This allows us to use Eq \ref{entropyeq}, which in turn is maximized when every state is equally likely.
That is, when the sequence is totally random.
If length of the sequence is $k$, then each $p( x ) = \frac 1 k$ and the maximum entropy is $\log_2( k )$
Hence, the maximum entropy increases as $k$ increases.
Increasing the entropy does not necessarily mean the model's performance will degrade; in fact, for the region supported by the size of the dataset, the performace improves.
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌────────────────────────────────────────────────────────────────┤ Conclusion ├╌╌╌╌╌╌
\section{Conclusion}

% ┌─────────┤ • ├
The use of location data at a societal-level is generally a net positive: the optimization of public infrastructure or public health and emergency service responses are popular initiatives.
On the other hand, the negative effects of both societal-level and personal-level location data uses, e.g. state surveillance or stalking, are felt on the personal level.

% Perhaps with more data, the HMM could tolerate higher sequence sizes and the RMSE could be decreased further.
% However, in the scenario of getting spotted intermittently, it is not realistic for

The limitations for the analysis in this paper were not due to the nature of HMMs but due to practical limitations.
The need for redundancy during data collection meant that text messages were sent and that limited to collecting data every two minutes.
Those two minutes were the minimum repeat interval Tasker would allow without heavily penalizing my phone's battery life.
I had to choose between 6-8 hours of regular data collection instead of 1-2 hours of continuous collection.

The RF Regression had the lowest RMSE across all input and parameter combinations tried, although increasing the subsequence length vastly improved HMM and NN performance.
However, the cardinality of the dataset limited how much HMM and NN models could be improved.

If you want to predict someones next move, use a RF Regression.
Converting the RMSE from coordinates into distance, our best RF model had an average RMSE of 0.722 km, or about 7 city blocks.
This is not precise enough to track someone, but given the very limited data, it is satisfactory as a proof of concept.

If you want to measure entropy, a model based on state transitions is required.

Back to our goal of providing anonymizing strategies, if you are in a state whose corresponding row in the transition matrix has a high entropy, then the prediction in the model misses more information.
In other words, you are anonymizing yourself by ensuring your routes take you through locations with high transition entropy.
For example, if am spotted leaving my apartment, I am either going south to campus or north to the Roosevelt Red Line station.
However, from the station there is a nearly equal probability that I'll walk in any direction, so being spotted at the station does not carry the same information as getting spotted leaving my apartment.

To maximize entropy, one could use a wheel-and-spoke system like those used by airlines, wherein transportation hubs are designated and ever route must pass through a hub, even if it increases total travel time.
In our case, the hubs are locations of high entropy which can be computed as the rows of the transition matrix which have the highest transition entropies.

Nature has already figured this out: fish dilute their chances of predation by congregating in schools.
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌
% ┌──────────────────────────────────────────────────────────────────┤ Appendix ├╌╌╌╌╌╌
\section*{Appendix}

% ┌─────────┤ • ├
A Hidden Markov Model (HMM) is a statistical model used to describe systems that have observable data (outputs) that depend on internal states, which are hidden and not directly observable.
The following is a list of the key concepts in HMM.

\textbf{States}: A set of hidden states that the model transitions between over time.
The sequence of these states represents the internal structure of the process but is not directly observed.

% • Example: In nuclear physics, we do not directly observe the states of protons and neutrons within the nucleus, such as their spin orientations or energy levels.
% Instead, these hidden states are inferred from experimental data, like emitted radiation.

\textbf{Observations}: The observable data generated by the system.
These are related to the hidden states, but the relationship is probabilistic.

% • Example: We observe gamma rays or beta particles emitted or absorbed by a nucleus.
% These emissions are influenced by the hidden quantum states of the nucleus (e.g., excited states or decay modes).

\textbf{Transition Probabilities}: The probabilities of moving from one hidden state to another.

% • Example: A nucleus in a specific excited state can probabilistically transition to another state, such as through proton-neutron conversion during beta decay.
% Inside a stable nucleus, this probability is finite, but for a free proton in isolation, the transition probability is so low that it practically never occurs within the age of the universe.

\textbf{Emission Probabilities}: The probabilities of observing a specific output given a particular hidden state.

% • Example: If the nucleus is in a specific excited state, there is a certain probability that it will emit gamma radiation of a specific energy.
% These probabilities are determined by the nuclear structure and selection rules in quantum mechanics.

\textbf{Initial Probabilities}: The probabilities of the system starting in each of the hidden states.

% • Example: The likelihood of a nucleus starting in an excited state versus a ground state can depend on how the nucleus was formed, such as through a nuclear reaction or radioactive decay.

What follows are the relevant definitions used in the paper's entropy discussion.

\textbf{Shannon Entropy}: measures the average uncertainty in a probability distribution of states.
\begin{equation}
    \label{entropyeq}
    H( X ) ≔ - \sum_{x ∈ X}p( x )\log_2 p( x ),
\end{equation}
The entropy of a sequence represents the average amount of information that could be encoded by the sequence.
For example, four letter words in English could take 26 states per letter, so there are $26^4$ possible permutations.
However, not every permutation is equally likely.
There are more consonants than vowels, so a word with 3 consonants and 1 vowel is less surprising than a word with 1 consonant and 3 vowels.
That is, not every sequence carries the same amount of information.
We want to compare the entropy of "everyday" location sequences to completely random location sequences to see how unpredictable an average person is.
After all, the higher the entropy, the lower the amount of information that can be extracted from the sequence.
This is a consequence of the definition of entropy as the average amount of uncertainty in a state.

\textbf{Transition Entropy}: a specific form of entropy that measures uncertainty associated with transitioning from one state to another in a system.
\begin{equation}
    \label{transition_entropy}
    H_{transition} = -\sum_i p( s_i )\sum_j p( s_j|s_i )\log_2 p( s_j|s_i )
\end{equation}
% └─────────┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
% └─────────────────────────────────────────────────────────────────────────────┼╌╌╌╌╌╌╌╌

\end{document}
